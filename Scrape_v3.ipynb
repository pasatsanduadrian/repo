{"cells":[{"cell_type":"markdown","id":"c424a04d","metadata":{"id":"c424a04d"},"source":["# Notebook Overview\n","This notebook includes setup and implementation for web scraping tasks. Below is the structured layout of the notebook for better understanding and readability."]},{"cell_type":"markdown","id":"96c806b4","metadata":{"id":"96c806b4"},"source":["## GeckoDriver and Selenium Setup Script"]},{"cell_type":"markdown","source":["This script is specifically designed for setting up the environment for web scraping using Selenium in Python. It focuses on installing and configuring necessary components to control a Firefox browser through Selenium.\n","\n","Key Features:\n","\n","    System and Browser Setup:\n","      Updates the system's package list.\n","      Installs Firefox, the web browser to be automated.\n","    GeckoDriver Installation:\n","      Downloads and extracts GeckoDriver, which is essential for Selenium to interact with Firefox.\n","      Sets executable permissions for GeckoDriver.\n","      Moves GeckoDriver to a system directory for easy access.\n","    Selenium Installation:\n","      Installs Selenium, a powerful tool for browser automation.\n","    Verification:   \n","      Checks the installation of GeckoDriver.\n","      Verifies the installed version of Firefox.\n","    Purpose:\n","      The script is tailored for preparing a Python environment for web scraping tasks.\n","      It ensures that all necessary components, particularly GeckoDriver and Selenium, are correctly installed and configured.\n","      This setup is crucial for automating and controlling the Firefox browser in web scraping applications.\n","\n","\n","\n"],"metadata":{"id":"wObxMl-AkpJO"},"id":"wObxMl-AkpJO"},{"cell_type":"code","execution_count":null,"id":"8d3f7664","metadata":{"id":"8d3f7664"},"outputs":[],"source":["!apt-get update\n","!apt-get install firefox\n","!wget https://github.com/mozilla/geckodriver/releases/download/v0.29.0/geckodriver-v0.29.0-linux64.tar.gz\n","!tar -xvzf geckodriver-v0.29.0-linux64.tar.gz\n","!chmod +x geckodriver\n","!mv geckodriver /usr/local/bin/\n","\n","\n","!pip install selenium\n","\n","!wget https://github.com/mozilla/geckodriver/releases/download/v0.34.0/geckodriver-v0.34.0-linux64.tar.gz\n","!tar -xvzf geckodriver-v0.34.0-linux64.tar.gz\n","!chmod +x geckodriver\n","!mv geckodriver /usr/local/bin/\n","\n","!tar -xvzf geckodriver-v0.34.0-linux64.tar.gz\n","!chmod +x geckodriver\n","!mv geckodriver /usr/local/bin/\n","\n","!which geckodriver\n","!firefox --version"]},{"cell_type":"markdown","id":"6fb26c7f","metadata":{"id":"6fb26c7f"},"source":["## Web Content Scraping Script\n","\n"]},{"cell_type":"markdown","source":["This Python script is designed for scraping specific content from web pages using the requests library and BeautifulSoup. It focuses on extracting structured data from a list of URLs.\n","\n","Key Features:\n","\n","    URLs Processing:\n","        Maintains a list of URLs to be scraped.\n","        Iterates over each URL to fetch its content.\n","\n","    Page Content Fetching:\n","        Uses the requests library to retrieve the HTML content of each page.\n","\n","    Data Extraction with BeautifulSoup:\n","        Parses the HTML content using BeautifulSoup.\n","        Extracts specific data based on HTML tags and classes.\n","\n","    Content Organization:\n","        Stores the extracted content in a structured format (dictionary).\n","        Handles different types of HTML elements like paragraphs and lists.\n","\n","    Output Display:\n","        Prints the extracted information in a readable format.\n","        Includes a separator for clarity between different scraped contents.\n","\n","    Purpose:\n","        The script is tailored for efficiently scraping and organizing information from multiple web pages.\n","        It demonstrates the use of Python's requests and BeautifulSoup for web scraping, showcasing the ability to handle and parse HTML content to extract meaningful data.\n","        This script is particularly useful for gathering structured information from various web sources."],"metadata":{"id":"8E5CJX2qoKO4"},"id":"8E5CJX2qoKO4"},{"cell_type":"code","execution_count":null,"id":"e2e2b91d","metadata":{"id":"e2e2b91d"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","\n","# List of links to scrape\n","links = [\n","    'https://ec.europa.eu/info/funding-tenders/opportunities/portal/screen/opportunities/topic-details/horizon-cl3-2024-bm-01-02',\n","    # ... Add all other links as needed\n","]\n","\n","# Iterate over each link to scrape information\n","for link in links:\n","    # Fetch the page content\n","    response = requests.get(link)\n","    soup = BeautifulSoup(response.content, 'html.parser')\n","\n","    # Initialize a dictionary to store the sections' content\n","    sections_content = {}\n","\n","    # Loop through each description tag and capture the content\n","    for tag in soup.find_all('span', class_='topicdescriptionkind'):\n","        section_title = tag.get_text(strip=True)\n","        content = ''\n","\n","        # Attempt to find the next element that contains text, this could be a paragraph, list, or another tag\n","        next_element = tag.find_next()\n","\n","        # Check if the next element is a paragraph or a list\n","        if next_element.name == 'p':\n","            content = next_element.get_text(strip=True)\n","        elif next_element.name == 'ul':\n","            # Combine all list items into a single string\n","            content = ' '.join(li.get_text(strip=True) for li in next_element.find_all('li'))\n","\n","        # Store the content in the dictionary using the section title as the key\n","        sections_content[section_title] = content\n","\n","    # Print the topic and its corresponding sections' content\n","    print(f\"Topic: {link.split('/')[-1]}\")\n","    for title, text in sections_content.items():\n","        print(f\"{title}: {text}\")\n","    print(\"\\n\" + \"-\"*80 + \"\\n\")  # Print a separator line\n"]},{"cell_type":"markdown","id":"ff9a7d3f","metadata":{"id":"ff9a7d3f"},"source":["## Headless Chrome Web Scraping Script\n","\n"]},{"cell_type":"markdown","source":["Headless Chrome Web Scraping Script\n","\n","This Python script is designed for web scraping using Selenium with a headless Chrome browser. It's configured to run in environments like Google Colab, where a graphical user interface is not available.\n","\n","Key Features:\n","\n","    Selenium WebDriver Setup:\n","        Imports necessary Selenium components for web automation.\n","        Initializes the Chrome WebDriver.\n","    Headless Chrome Configuration:\n","        Configures Chrome to run in headless mode (no GUI).\n","        Sets additional arguments for compatibility with environments like Colab (--no-sandbox, --disable-dev-shm-usage).\n","    Web Page Interaction:\n","        Uses the WebDriver to navigate to a specified URL.\n","        Demonstrates how to interact with web elements (e.g., retrieving the page title).\n","    Scraping Task Execution:\n","        The script is structured to perform web scraping tasks on the visited page.\n","    Clean Up:\n","        Properly closes the Chrome driver after completing tasks to free resources.\n","    Purpose:\n","      The script is specifically designed for automated web scraping tasks in headless environments.\n","      It showcases how to configure and use Selenium with a headless Chrome browser, making it suitable for server-side scraping tasks or in environments without a display.\n","      This approach is essential for efficient, automated data extraction from web pages in non-GUI contexts."],"metadata":{"id":"zyYqI-rTqK0O"},"id":"zyYqI-rTqK0O"},{"cell_type":"code","execution_count":null,"id":"569c57aa","metadata":{"id":"569c57aa"},"outputs":[],"source":["from selenium import webdriver\n","from selenium.webdriver.chrome.options import Options\n","\n","# Set up Chrome options for headless browsing in Colab\n","options = Options()\n","options.add_argument('--headless')\n","options.add_argument('--no-sandbox')\n","options.add_argument('--disable-dev-shm-usage')\n","\n","# Initialize the Chrome driver with the specified options\n","driver = webdriver.Chrome(options=options)\n","\n","# Now you can use the driver object to visit pages, interact with elements, etc.\n","# For example, to visit a webpage:\n","driver.get('https://ec.europa.eu/info/funding-tenders/opportunities/portal/screen/opportunities/topic-details/horizon-cl3-2024-bm-01-01?keywords=HORIZON-CL3-2024&tenders=false&openForSubmission=false&sortBy=identifier&pageSize=25')\n","# Print the title of the page\n","print(driver.title)\n","\n","# Do your scraping tasks...\n","\n","# Don't forget to close the driver after your tasks are done\n","driver.quit()\n"]},{"cell_type":"markdown","id":"b81cb2cf","metadata":{"id":"b81cb2cf"},"source":["## Web Scraping Script Description\n","\n"]},{"cell_type":"markdown","source":["This Python script is designed for **web scraping using Selenium and BeautifulSoup**. It automates a web browser to extract specific information from a webpage.\n","\n","Key Features:\n","    \n","    Selenium Installation and Setup:\n","      Installs and configures Selenium for browser automation.\n","      Sets up GeckoDriver and ChromeDriver for Firefox and Chrome browsers.\n","\n","    Web Browser Configuration:\n","      Customizes Chrome browser settings for the scraping task.\n","   \n","    Web Scraping Process:\n","      Automates navigation to a specified URL.\n","      Waits for the necessary page elements to load.\n","      Parses HTML content using BeautifulSoup.\n","      Extracts and prints targeted information from the webpage.\n","      Handles timeouts and exceptions efficiently.\n","      Closes the browser driver post-scraping.\n","    Purpose:\n","      The script demonstrates a comprehensive approach to web scraping, showcasing techniques for browser automation,\n","      dynamic content handling, and structured data extraction from webpages."],"metadata":{"id":"yxbcxsM1e5-_"},"id":"yxbcxsM1e5-_"},{"cell_type":"code","execution_count":null,"id":"85e38996","metadata":{"id":"85e38996"},"outputs":[],"source":["!pip install --upgrade selenium\n","\n","!apt-get install firefox\n","!wget https://github.com/mozilla/geckodriver/releases/download/v0.29.0/geckodriver-v0.29.0-linux64.tar.gz\n","!tar -xvzf geckodriver-v0.29.0-linux64.tar.gz\n","!chmod +x geckodriver\n","!mv geckodriver /usr/local/bin/\n","\n","!tar -xvzf geckodriver-v0.29.0-linux64.tar.gz\n","!chmod +x geckodriver\n","!mv geckodriver /usr/local/bin/\n","\n","!rm /usr/local/bin/geckodriver\n","!wget https://github.com/mozilla/geckodriver/releases/download/v0.34.0/geckodriver-v0.34.0-linux64.tar.gz\n","!tar -xvzf geckodriver-v0.34.0-linux64.tar.gz\n","!chmod +x geckodriver\n","!mv geckodriver /usr/local/bin/\n","\n","\n","# Install Selenium\n","!pip install selenium\n","\n","# Download the latest chromedriver and unzip it\n","!apt-get update # to update ubuntu to correctly run apt install\n","!apt install chromium-chromedriver\n","!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n","\n","import sys\n","sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n","\n","from selenium import webdriver\n","from selenium.webdriver.chrome.service import Service\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.support.ui import WebDriverWait\n","from selenium.webdriver.support import expected_conditions as EC\n","from selenium.common.exceptions import TimeoutException\n","from bs4 import BeautifulSoup\n","import time\n","\n","\n","\n","# Set up options for Chrome, this time without headless mode\n","chrome_options = webdriver.ChromeOptions()\n","# chrome_options.add_argument('--headless')  # Comment this line out or remove it\n","chrome_options.add_argument('--no-sandbox')\n","chrome_options.add_argument('--disable-dev-shm-usage')\n","chrome_options.add_argument(\"--disable-images\")\n","chrome_options.add_argument(\"--disable-javascript\")\n","chrome_options.add_argument('--disable-gpu')\n","chrome_options.add_argument('--window-size=1920,1080')\n","\n","\n","# Specify the path to the Chrome binary\n","chrome_options.binary_location = \"/usr/bin/google-chrome\"\n","\n","# Initialize the Chrome driver\n","driver = webdriver.Chrome(options=chrome_options)\n","\n","\n","# URL of the webpage you want to scrape\n","url = \"https://ec.europa.eu/info/funding-tenders/opportunities/portal/screen/opportunities/topic-details/horizon-cl3-2024-fct-01-05?keywords=CL3-2024&openForSubmission=false&programmePeriod=2021%20-%202027&frameworkProgramme=43108390\"\n","\n","# Use the driver to navigate to the page\n","driver.get(url)\n","\n","try:\n","    # Wait for the page content to load\n","    WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n","    print(\"Page loaded successfully.\")\n","except TimeoutException:\n","    print(\"Timed out waiting for page content to load.\")\n","\n","# try:\n","\n","# # Increase timeout and try a different approach to locate the element\n","#     WebDriverWait(driver, 90).until(EC.visibility_of_element_located((By.XPATH, \"//eui-card-header-title[contains(text(), 'Topic description')]\")))\n","\n","#     # Now get the page source and create a BeautifulSoup object\n","#     soup = BeautifulSoup(driver.page_source, 'html.parser')\n","\n","#     # Function to extract information\n","#     def extract_information(soup):\n","#         topic_info = {}\n","#         sections_to_scrape = [\"ExpectedOutcome\", \"Scope\"]\n","\n","#         # Find the 'Topic description' title\n","#         topic_description_title = soup.find('eui-card-header-title', string=\"Topic description\")\n","#         if topic_description_title:\n","#             topic_description_content = topic_description_title.find_next('div', class_='showMore--three-lines')\n","#             topic_info[\"Topic description\"] = topic_description_content.get_text(strip=True) if topic_description_content else \"Content not found\"\n","\n","#         # Find 'ExpectedOutcome' and 'Scope' elements and their respective contents\n","#         for section in sections_to_scrape:\n","#             section_content = soup.find('span', class_='topicdescriptionkind', string=section)\n","#             if section_content:\n","#                 content = section_content.find_next('p')\n","#                 topic_info[section] = content.get_text(strip=True) if content else 'Content not found'\n","\n","#         return topic_info\n","\n","#     # Call the function with the BeautifulSoup object\n","#     scraped_data = extract_information(soup)\n","\n","#     # Print the scraped data\n","#     for key, value in scraped_data.items():\n","#         print(f\"{key}: {value}\\n\")\n","\n","# except TimeoutException:\n","#     print(\"Timed out waiting for page to load\")\n","# finally:\n","#     # Close the driver\n","#     driver.quit()\n","\n","\n","try:\n","\n","    print(\"Navigated to URL.\")\n","\n","    # Alternative method to find and click the 'Show more' button\n","    # Update this XPath according to the button's attributes or text\n","    show_more_button_xpath = \"//button[contains(text(), 'Show more')]\"  # Example XPath\n","    WebDriverWait(driver, 30).until(EC.element_to_be_clickable((By.XPATH, show_more_button_xpath)))\n","    driver.find_element(By.XPATH, show_more_button_xpath).click()\n","    print(\"'Show more' button clicked.\")\n","\n","\n","    # Wait for the visibility of the title 'Topic description'\n","    WebDriverWait(driver, 30).until(EC.visibility_of_element_located((By.XPATH, \"//eui-card-header-title[contains(text(), 'Topic description')]\")))\n","    print(\"Topic description is visible.\")\n","\n","    # Now get the page source and create a BeautifulSoup object\n","    soup = BeautifulSoup(driver.page_source, 'html.parser')\n","\n","    # Function to extract information\n","    def extract_information(soup):\n","        topic_info = {}\n","        sections_to_scrape = [\"ExpectedOutcome\", \"Scope\"]\n","\n","        # Find the 'Topic description' title\n","        topic_description_title = soup.find('eui-card-header-title', string=\"Topic description\")\n","        if topic_description_title:\n","            topic_description_content = topic_description_title.find_next('div', class_='showMore--three-lines')\n","            topic_info[\"Topic description\"] = topic_description_content.get_text(strip=True) if topic_description_content else \"Content not found\"\n","\n","        # Find 'ExpectedOutcome' and 'Scope' elements and their respective contents\n","        for section in sections_to_scrape:\n","            section_content = soup.find('span', class_='topicdescriptionkind', string=section)\n","            if section_content:\n","                content = section_content.find_next('p')\n","                topic_info[section] = content.get_text(strip=True) if content else 'Content not found'\n","\n","        return topic_info\n","\n","    # Call the function with the BeautifulSoup object\n","    scraped_data = extract_information(soup)\n","    print(\"Scraped data extracted.\")\n","\n","    # Print the scraped data\n","    for key, value in scraped_data.items():\n","        print(f\"{key}: {value}\\n\")\n","\n","except TimeoutException:\n","    print(\"Timed out waiting for page to load or element to be found\")\n","except Exception as e:\n","    print(f\"An error occurred: {e}\")\n","finally:\n","    # Close the driver\n","    driver.quit()"]},{"cell_type":"markdown","id":"2e7b68c3","metadata":{"id":"2e7b68c3"},"source":["### This cell imports necessary libraries and sets up the environment for the task."]},{"cell_type":"code","execution_count":null,"id":"7e98df28","metadata":{"id":"7e98df28"},"outputs":[],"source":["import requests\n","\n","# URL of the JSON file\n","url = 'https://ec.europa.eu/info/funding-tenders/opportunities/data/topicDetails/horizon-cl3-2024-fct-01-05.json'\n","\n","# Send a GET request\n","response = requests.get(url)\n","\n","# Parse the JSON response\n","data = response.json()\n","\n","# Print the top-level keys\n","print(data.keys())\n"]},{"cell_type":"markdown","id":"3242a951","metadata":{"id":"3242a951"},"source":["### This cell imports necessary libraries and sets up the environment for the task."]},{"cell_type":"code","execution_count":null,"id":"c2f0300e","metadata":{"id":"c2f0300e"},"outputs":[],"source":["import requests\n","\n","# URL of the JSON file\n","url = 'https://ec.europa.eu/info/funding-tenders/opportunities/data/topicDetails/horizon-cl3-2024-fct-01-05.json'\n","\n","# Send a GET request\n","response = requests.get(url)\n","\n","# Parse the JSON response\n","data = response.json()\n","\n","# Access the 'TopicDetails' key\n","topic_details = data['TopicDetails']\n","\n","# Now, you need to find the correct sub-key under 'TopicDetails' that contains the \"Topic description\"\n","# If you're not sure what the sub-key is, you can print out the keys within 'TopicDetails' to explore:\n","print(topic_details.keys())\n","\n","# Once you identify the correct sub-key, you can access the \"Topic description\" like this:\n","# topic_description = topic_details['the_sub_key_here']\n","# print(topic_description)\n"]},{"cell_type":"markdown","id":"23324ba5","metadata":{"id":"23324ba5"},"source":["### This cell imports necessary libraries and sets up the environment for the task."]},{"cell_type":"code","execution_count":null,"id":"e124d0cc","metadata":{"id":"e124d0cc"},"outputs":[],"source":["import requests\n","\n","# URL of the JSON file\n","url = 'https://ec.europa.eu/info/funding-tenders/opportunities/data/topicDetails/horizon-cl3-2024-fct-01-03.json'\n","\n","# Send a GET request\n","response = requests.get(url)\n","\n","# Parse the JSON response\n","data = response.json()\n","\n","# Extract the 'Topic description' from 'destinationDetails' key\n","topic_description = data['TopicDetails']\n","\n","# Print or process the 'Topic description'\n","print(topic_description)\n"]},{"cell_type":"markdown","id":"74912a83","metadata":{"id":"74912a83"},"source":["### This cell imports necessary libraries and sets up the environment for the task."]},{"cell_type":"code","execution_count":null,"id":"51597ae7","metadata":{"id":"51597ae7"},"outputs":[],"source":["import requests\n","import pandas as pd\n","from datetime import datetime\n","\n","# URL of the JSON file\n","url = 'https://ec.europa.eu/info/funding-tenders/opportunities/data/topicDetails/horizon-cl3-2024-fct-01-05.json'\n","\n","# Send a GET request\n","response = requests.get(url)\n","\n","# Parse the JSON response\n","data = response.json()['TopicDetails']\n","\n","# Extract specific data\n","call_id = data['identifier']\n","call_title = data['callTitle']\n","type_of_action = data['actions'][0]['types'][0]['typeOfAction']\n","type_of_mga = data['actions'][0]['types'][0]['typeOfMGA'][0]['abbreviation']\n","\n","# Converting dates from timestamp to readable format\n","planned_opening_date = data['actions'][0]['plannedOpeningDate']\n","deadline_date = data['actions'][0]['deadlineDates'][0] + \" 17:00:00 Brussels time\"\n","\n","\n","# Create a DataFrame for table format\n","df = pd.DataFrame({\n","    'Call ID': [call_id],\n","    'Call Title': [call_title],\n","    'Type of Action': [type_of_action],\n","    'Type of MGA': [type_of_mga],\n","    'Planned Opening Date': [planned_opening_date],\n","    'Deadline Date': [deadline_date]\n","})\n","\n","# Print the DataFrame\n","print(df)\n"]},{"cell_type":"markdown","id":"79a21d8c","metadata":{"id":"79a21d8c"},"source":["## JSON Data Retrieval and Parsing Script\n","\n"]},{"cell_type":"markdown","source":["This Python script is designed for fetching and parsing JSON data from a specified URL using the requests library. It focuses on retrieving structured JSON data from a web source and extracting key information.\n","\n","Key Features:\n","\n","    URL Specification:\n","        Defines the URL of the JSON data to be fetched.\n","    Data Fetching:\n","        Uses the requests library to send a GET request to the specified URL.\n","    JSON Parsing:\n","        Parses the JSON response from the request.\n","    Data Inspection:\n","        Prints the top-level keys of the JSON data to provide an overview of its structure.\n","    Purpose:\n","      The script is tailored for simple and efficient retrieval and parsing of JSON data from web sources.\n","      It demonstrates the use of Python's requests library for network requests and JSON handling, showcasing the ability\n","      to easily access and inspect structured data from the internet.\n","      This script is particularly useful for applications that require automated data fetching and parsing from online JSON sources."],"metadata":{"id":"m-xcRGKKrd84"},"id":"m-xcRGKKrd84"},{"cell_type":"code","execution_count":null,"id":"fdad7226","metadata":{"id":"fdad7226"},"outputs":[],"source":["import requests\n","\n","# URL of the JSON file\n","url = 'https://ec.europa.eu/info/funding-tenders/opportunities/data/topicDetails/horizon-cl3-2024-fct-01-05.json'\n","\n","# Send a GET request\n","response = requests.get(url)\n","\n","# Parse the JSON response\n","data = response.json()\n","\n","# Print the top-level keys\n","print(data.keys())\n"]},{"cell_type":"markdown","id":"9db288e4","metadata":{"id":"9db288e4"},"source":["## JSON Data Retrieval and Specific Content Extraction Script\n","\n"]},{"cell_type":"markdown","source":["This Python script is tailored for fetching JSON data from a URL and extracting specific information from it using the requests library. It focuses on accessing a particular section of the JSON data structure.\n","\n","Key Features:\n","\n","    URL Definition:\n","        Specifies the URL of the JSON file to be retrieved.\n","    GET Request Execution:\n","        Sends a GET request to the URL to fetch the JSON data.\n","    JSON Data Parsing:\n","        Parses the received JSON response.\n","    Accessing Specific Data:\n","        Extracts the 'TopicDetails' section from the JSON data.\n","        Prints the keys within 'TopicDetails' to facilitate exploration and identification of the relevant sub-key.\n","    Targeted Data Extraction:\n","        Provides a template for accessing a specific sub-key (e.g., \"Topic description\") once identified.\n","    Purpose:\n","        The script is designed for scenarios where specific pieces of information need to be extracted from structured JSON data obtained from a web source.\n","        It demonstrates the process of fetching, parsing, and drilling down into JSON data to access detailed content.\n","        This approach is valuable for applications that require targeted data extraction from complex JSON structures, particularly in data analysis or integration tasks."],"metadata":{"id":"cqMc2HEfsGcJ"},"id":"cqMc2HEfsGcJ"},{"cell_type":"code","execution_count":null,"id":"19b4f110","metadata":{"id":"19b4f110"},"outputs":[],"source":["import requests\n","\n","# URL of the JSON file\n","url = 'https://ec.europa.eu/info/funding-tenders/opportunities/data/topicDetails/horizon-cl3-2024-fct-01-05.json'\n","\n","# Send a GET request\n","response = requests.get(url)\n","\n","# Parse the JSON response\n","data = response.json()\n","\n","# Access the 'TopicDetails' key\n","topic_details = data['TopicDetails']\n","\n","# Now, you need to find the correct sub-key under 'TopicDetails' that contains the \"Topic description\"\n","# If you're not sure what the sub-key is, you can print out the keys within 'TopicDetails' to explore:\n","print(topic_details.keys())\n","\n","# Once you identify the correct sub-key, you can access the \"Topic description\" like this:\n","# topic_description = topic_details['the_sub_key_here']\n","# print(topic_description)\n"]},{"cell_type":"markdown","id":"76ee0798","metadata":{"id":"76ee0798"},"source":["## JSON Data Retrieval and DataFrame Creation Script\n","\n"]},{"cell_type":"markdown","source":["This Python script is designed for fetching JSON data from a specified URL, extracting specific information, and organizing it into a pandas DataFrame. It uses the requests library for data retrieval and pandas for data structuring.\n","\n","Key Features:\n","\n","    URL and Data Fetching:\n","        Specifies the URL of the JSON file.\n","        Fetches the JSON data using a GET request.\n","    JSON Parsing and Data Extraction:\n","        Parses the JSON response to access the 'TopicDetails'.\n","        Extracts specific details like call ID, title, type of action, and type of MGA.\n","    Date Formatting:\n","        Converts timestamps into a readable date format.\n","    DataFrame Creation:\n","        Utilizes pandas to create a DataFrame for structured data representation.\n","        Organizes extracted data into a tabular format.\n","    Data Display:\n","        Prints the DataFrame for review or further analysis."],"metadata":{"id":"p4Fr80sQs2KQ"},"id":"p4Fr80sQs2KQ"},{"cell_type":"code","execution_count":null,"id":"eb3c0e12","metadata":{"id":"eb3c0e12"},"outputs":[],"source":["import requests\n","import pandas as pd\n","from datetime import datetime\n","\n","# URL of the JSON file\n","url = 'https://ec.europa.eu/info/funding-tenders/opportunities/data/topicDetails/horizon-cl3-2024-fct-01-05.json'\n","\n","# Send a GET request\n","response = requests.get(url)\n","\n","# Parse the JSON response\n","data = response.json()['TopicDetails']\n","\n","# Extract specific data\n","call_id = data['identifier']\n","call_title = data['callTitle']\n","type_of_action = data['actions'][0]['types'][0]['typeOfAction']\n","type_of_mga = data['actions'][0]['types'][0]['typeOfMGA'][0]['abbreviation']\n","\n","# Converting dates from timestamp to readable format\n","planned_opening_date = data['actions'][0]['plannedOpeningDate']\n","deadline_date = data['actions'][0]['deadlineDates'][0] + \" 17:00:00 Brussels time\"\n","\n","\n","# Create a DataFrame for table format\n","df = pd.DataFrame({\n","    'Call ID': [call_id],\n","    'Call Title': [call_title],\n","    'Type of Action': [type_of_action],\n","    'Type of MGA': [type_of_mga],\n","    'Planned Opening Date': [planned_opening_date],\n","    'Deadline Date': [deadline_date]\n","})\n","\n","# Print the DataFrame\n","print(df)"]},{"cell_type":"markdown","id":"5322a502","metadata":{"id":"5322a502"},"source":["## POST Request and Data Processing Script\n","\n"]},{"cell_type":"markdown","source":["This Python script is designed for sending a POST request to a specified API, processing the JSON response, and organizing the data into a pandas DataFrame, which is then saved to an Excel file. It uses the requests library for the API call and pandas for data handling.\n","\n","**Adauga Call Id in Excel/ Sheet1**\n","\n","Key Features:\n","\n","    API Interaction:\n","        Defines the URL for a POST request to an API.\n","        Sends the POST request and checks the response status.\n","    JSON Response Handling:\n","        Parses the JSON response upon a successful request.\n","        Extracts relevant data from the \"results\" array.\n","    Data Processing:\n","        Processes and filters the data, avoiding duplicates.\n","        Converts content to lowercase for consistency.\n","    Data Sorting and DataFrame Creation:\n","        Sorts the entries based on identifiers.\n","        Creates a DataFrame with the sorted data.\n","    Excel File Generation:\n","        Saves the DataFrame to an Excel file for external use.\n","    Data Display:\n","        Prints the sorted entries for immediate review."],"metadata":{"id":"w_7MwHYGtY6P"},"id":"w_7MwHYGtY6P"},{"cell_type":"code","execution_count":null,"id":"7e2bdcfc","metadata":{"id":"7e2bdcfc"},"outputs":[],"source":["import requests\n","import json\n","import pandas as pd\n","\n","# Define the URL for the POST request\n","url = 'https://api.tech.ec.europa.eu/search-api/prod/rest/search?apiKey=SEDIA&text=*CL3-2024*&pageSize=50&pageNumber=1'\n","\n","# Make the POST request\n","response = requests.post(url)\n","\n","# Check the response status code\n","if response.status_code == 200:\n","    # Request was successful, parse the JSON response\n","    json_data = json.loads(response.text)\n","\n","    # Extract information from the \"results\" array\n","    results = json_data.get(\"results\", [])\n","\n","    # Create a list to store entries and a set to keep track of processed identifiers\n","    entries = []\n","    processed_identifiers = set()\n","\n","    for result in results:\n","        metadata = result.get(\"metadata\", {})\n","        identifiers = metadata.get(\"identifier\", [])\n","        content = result.get(\"content\", \"\")\n","\n","        # Check if identifiers exist and are not empty\n","        if identifiers:\n","            identifier = identifiers[0]  # Use the first identifier\n","            content = content.lower()  # Convert content to lowercase\n","\n","            # Check if the identifier has already been processed\n","            if identifier not in processed_identifiers:\n","                # Append the entry to the list and mark the identifier as processed\n","                entries.append((identifier.lower(), content))\n","                processed_identifiers.add(identifier)\n","\n","    # Sort entries by identifier content\n","    sorted_entries = sorted(entries, key=lambda x: x[0])\n","\n","    # Create a DataFrame from the sorted entries\n","    df = pd.DataFrame(sorted_entries, columns=['Call-Id', 'Content'])\n","\n","    # Save the DataFrame to an Excel file in Sheet 1\n","    with pd.ExcelWriter('eu_topic_details.xlsx', engine='openpyxl') as writer:\n","        df.to_excel(writer, sheet_name='Sheet1', index=False)\n","\n","    # Print the sorted entries\n","    for idx, entry in enumerate(sorted_entries, start=1):\n","        identifier, content = entry\n","        print(f\"{idx}. Identifier: {identifier}\")\n","        print(f\"   Content: {content}\")\n","\n","else:\n","    # Request failed\n","    print(f'POST request failed with status code {response.status_code}')"]},{"cell_type":"markdown","source":["## Web Data Extraction and DataFrame Compilation Script\n","\n"],"metadata":{"id":"2dCW7WRbtxqf"},"id":"2dCW7WRbtxqf"},{"cell_type":"markdown","source":["This Python script is designed for extracting specific information from web sources using identifiers from an Excel file, and then organizing this data into a structured pandas DataFrame. It uses requests for web requests, BeautifulSoup for HTML parsing, and pandas for data handling.\n","\n","Key Features: **Parseaza datele de interes in Excel / Sheet2**\n","\n","    Excel File Reading:\n","        Reads identifiers from an Excel file using pandas.\n","    Web Requests and Data Extraction:\n","        Iterates through each identifier to construct and send GET requests to specific URLs.\n","        Parses JSON responses to extract detailed information.\n","    HTML Content Parsing:\n","        Utilizes BeautifulSoup to parse HTML content within the JSON data.\n","        Extracts and cleans specific sections like 'Expected Outcome' and 'Scope'.\n","    DataFrame Creation and Updating:\n","        Compiles extracted data into a pandas DataFrame.\n","        Appends new data to the DataFrame for each identifier.\n","    Error Handling:\n","        Includes exception handling for robust processing.\n","    Excel File Writing:\n","        Saves the compiled data into a new sheet in the Excel file."],"metadata":{"id":"0mkbgeb0uTW2"},"id":"0mkbgeb0uTW2"},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","# Read the Excel file with identifiers from Sheet1\n","df_identifiers = pd.read_excel('eu_topic_details.xlsx', sheet_name='Sheet1')\n","\n","# Create an empty DataFrame to store the results\n","df_results = pd.DataFrame(columns=[\n","    'Call ID',\n","    'Call Title',\n","    'Type of Action',\n","    'Type of MGA',\n","    'Planned Opening Date',\n","    'Deadline Date',\n","    'Expected Outcome',\n","    'Scope'\n","])\n","\n","# Iterate through identifiers\n","for index, row in df_identifiers.iterrows():\n","    identifier = row['Call-Id']\n","\n","    # Construct the URL with the identifier\n","    url = f'https://ec.europa.eu/info/funding-tenders/opportunities/data/topicDetails/{identifier}.json'\n","\n","    # Send a GET request\n","    response = requests.get(url)\n","\n","    try:\n","        # Parse the JSON response\n","        data = response.json()\n","\n","        # Extract the required fields\n","        call_id = data['TopicDetails']['identifier']\n","        call_title = data['TopicDetails']['callTitle']\n","        type_of_action = data['TopicDetails']['actions'][0]['types'][0]['typeOfAction']\n","        type_of_mga = data['TopicDetails']['actions'][0]['types'][0]['typeOfMGA'][0]['description']\n","        planned_opening_date = data['TopicDetails']['actions'][0]['plannedOpeningDate']\n","        deadline_date = data['TopicDetails']['actions'][0]['deadlineDates'][0] + \" 17:00:00 Brussels time\"\n","\n","        # Extracting and cleaning the ExpectedOutcome and Scope content\n","        description_html = data['TopicDetails']['description']\n","        soup = BeautifulSoup(description_html, 'html.parser')\n","\n","        # Extracting text content for ExpectedOutcome and Scope\n","        expected_outcome_tag = soup.find('span', class_='topicdescriptionkind', string='ExpectedOutcome')\n","        scope_tag = soup.find('span', class_='topicdescriptionkind', string='Scope')\n","\n","        expected_outcome = \"\"\n","        scope = \"\"\n","\n","        if expected_outcome_tag:\n","            # Find all <li> tags within the ExpectedOutcome section and concatenate their text\n","            expected_outcome_list = expected_outcome_tag.find_next('ul')\n","            if expected_outcome_list:\n","                expected_outcome_items = expected_outcome_list.find_all('li')\n","                expected_outcome = \"\\n\".join([item.get_text(strip=True) for item in expected_outcome_items])\n","\n","        if scope_tag:\n","            # Find the next <p> tag after the Scope section and get its text\n","            scope_paragraph = scope_tag.find_next('p')\n","            if scope_paragraph:\n","                scope = scope_paragraph.get_text(strip=True)\n","\n","        # Add the data to the DataFrame for results\n","        df_results = df_results.append({\n","            'Call ID': call_id,\n","            'Call Title': call_title,\n","            'Type of Action': type_of_action,\n","            'Type of MGA': type_of_mga,\n","            'Planned Opening Date': planned_opening_date,\n","            'Deadline Date': deadline_date,\n","            'Expected Outcome': expected_outcome,\n","            'Scope': scope\n","        }, ignore_index=True)\n","\n","    except Exception as e:\n","        print(f\"Error processing identifier {identifier}: {str(e)}\")\n","\n","# Save the results to an Excel file\n","with pd.ExcelWriter('eu_topic_details.xlsx', engine='openpyxl', mode='a') as writer:\n","    df_results.to_excel(writer, sheet_name='Sheet2', index=False)\n","\n","# Print the results DataFrame for Sheet 2\n","print(df_results)"],"metadata":{"id":"R6ekCibCtvPt"},"id":"R6ekCibCtvPt","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Budget Data Extraction and Compilation Script\n","\n"],"metadata":{"id":"1JR2lX73urUT"},"id":"1JR2lX73urUT"},{"cell_type":"markdown","source":["This Python script is designed for extracting budget-related information from a web source using identifiers from an Excel file and organizing this data into a structured pandas DataFrame. It uses requests for web requests and pandas for data handling and Excel file operations.\n","\n","Key Features:\n","\n","    Excel File Interaction:\n","        Reads identifiers from an existing Excel file.\n","        Prepares to save new data to the same file.\n","    Web Requests and JSON Parsing:\n","        Iterates through identifiers to construct and send GET requests.\n","        Parses JSON responses to extract budget information.\n","    Data Extraction and Processing:\n","        Extracts budget values, number of projects, and budget per project.\n","        Utilizes a custom function to handle budget value extraction.\n","    Unique Identifier Tracking:\n","        Ensures data uniqueness by tracking action identifiers.\n","    DataFrame Creation and Updating:\n","        Compiles extracted data into a new DataFrame.\n","        Appends new data for each unique action identifier.\n","    Error Handling:\n","        Implements exception handling for robust data processing.\n","    Excel File Writing:\n","        Adds the new DataFrame to a new sheet in the existing Excel file."],"metadata":{"id":"ZKAZ8-mOu-cD"},"id":"ZKAZ8-mOu-cD"},{"cell_type":"code","source":["import requests\n","import pandas as pd\n","\n","# Define the path to your Excel file\n","input_excel_path = '/content/eu_topic_details.xlsx'\n","output_excel_path = '/content/eu_topic_details.xlsx'  # Updated output path\n","\n","# Read the Excel file with identifiers from Sheet1\n","df_identifiers = pd.read_excel(input_excel_path, sheet_name='Sheet1')\n","\n","# Prepare a new DataFrame to store action identifiers and budget values\n","budget_data = pd.DataFrame(columns=['Action Identifier', 'Budget Value', 'Number of Projects', 'Budget Per Project'])\n","\n","# Define a function to extract the budget value as a number\n","def get_budget_value(budget_year_map):\n","    if '2024' in budget_year_map:\n","        return budget_year_map['2024']\n","    return 0\n","\n","# Set to keep track of unique Action Identifiers\n","unique_action_identifiers = set()\n","\n","# Process each link in the Excel file\n","for index, row in df_identifiers.iterrows():\n","    identifier = row['Call-Id']\n","\n","    # Construct the URL with the identifier\n","    url = f'https://ec.europa.eu/info/funding-tenders/opportunities/data/topicDetails/{identifier}.json'\n","\n","    # Send a GET request\n","    response = requests.get(url)\n","\n","    try:\n","        # Parse the JSON response\n","        data = response.json()\n","\n","        # Access data under 'TopicDetails' and then 'budgetOverviewJSONItem'\n","        budget_overview = data['TopicDetails'].get('budgetOverviewJSONItem', {}).get('budgetTopicActionMap', {})\n","\n","        for action_id, actions in budget_overview.items():\n","            for action in actions:\n","                # Extract the required fields\n","                action_identifier = action.get('action', '').split(' - ')[0]\n","                budget_value = get_budget_value(action.get('budgetYearMap', {}))\n","                num_projects = action.get('expectedGrants', 0)\n","                budget_per_project = action.get('minContribution', 0)\n","\n","                # Check for duplicate Action Identifiers\n","                if action_identifier not in unique_action_identifiers:\n","                    # Append the data to the new DataFrame\n","                    budget_data = budget_data.append({\n","                        'Action Identifier': action_identifier,\n","                        'Budget Value': budget_value,\n","                        'Number of Projects': num_projects,\n","                        'Budget Per Project': budget_per_project\n","                    }, ignore_index=True)\n","                    unique_action_identifiers.add(action_identifier)\n","\n","    except Exception as e:\n","        print(f\"Error processing identifier {identifier}: {str(e)}\")\n","\n","# Save the new DataFrame with action identifiers and budget values to the same Excel file in a new sheet (Sheet3)\n","with pd.ExcelWriter(output_excel_path, engine='openpyxl', mode='a') as writer:\n","    budget_data.to_excel(writer, sheet_name='Sheet3', index=False)\n","\n","print(\"Processing complete. Action identifiers and budget values added to 'Sheet3' in the same Excel file.\")"],"metadata":{"id":"iYOzKUcsu0Z0"},"id":"iYOzKUcsu0Z0","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","# Specify the path to your Google Drive folder\n","drive_path = '/content/drive/My Drive/'\n","\n","# List all items (files and folders) in your Google Drive root directory\n","drive_items = os.listdir(drive_path)\n","\n","# Filter and print only the folders\n","folders = [item for item in drive_items if os.path.isdir(os.path.join(drive_path, item))]\n","\n","# Print the list of folders\n","print(\"List of Folders in Google Drive:\")\n","for folder in folders:\n","    print(folder)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9K5oeXaj5ZnA","executionInfo":{"status":"ok","timestamp":1706694351336,"user_tz":-120,"elapsed":8,"user":{"displayName":"Adi Pasat","userId":"01384556905674102264"}},"outputId":"68b9306b-4157-4db1-bf2c-91e8a3d5597e"},"id":"9K5oeXaj5ZnA","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["List of Folders in Google Drive:\n","BEIA\n","KINETO\n","cursuri\n","Books\n","UPB\n","odbackup\n","GFoto\n","cardsdxi\n","Colab Notebooks\n","RST_A6.1\n","appsheet\n"]}]},{"cell_type":"code","source":["%cd '/content/drive/My Drive/Colab Notebooks/'\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P3OK_6vl5mZ6","executionInfo":{"status":"ok","timestamp":1706694478348,"user_tz":-120,"elapsed":7,"user":{"displayName":"Adi Pasat","userId":"01384556905674102264"}},"outputId":"2e185a19-7e1a-419d-a6a9-c4a8aee28c2a"},"id":"P3OK_6vl5mZ6","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Colab Notebooks\n"]}]},{"cell_type":"code","source":["rm -rf repo\n"],"metadata":{"id":"KPcZBnSn738H"},"id":"KPcZBnSn738H","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/pasatsanduadrian/repo.git\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZzVqSVov6x8e","executionInfo":{"status":"ok","timestamp":1706694789100,"user_tz":-120,"elapsed":1019,"user":{"displayName":"Adi Pasat","userId":"01384556905674102264"}},"outputId":"14867500-585a-4513-9350-83b9b095e143"},"id":"ZzVqSVov6x8e","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'repo'...\n","warning: You appear to have cloned an empty repository.\n"]}]},{"cell_type":"code","source":["%cd '/content/drive/My Drive/Colab Notebooks/'\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x4SPzjq28PWN","executionInfo":{"status":"ok","timestamp":1706695039682,"user_tz":-120,"elapsed":280,"user":{"displayName":"Adi Pasat","userId":"01384556905674102264"}},"outputId":"98eab387-b860-40b6-ee5b-b6dbbc138ad1"},"id":"x4SPzjq28PWN","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Colab Notebooks\n"]}]},{"cell_type":"code","source":["%cd repo  # Navigate to the 'repo' directory\n","!git add Scrape_v3.ipynb  # Add the file you want to commit\n","!git commit -m \"Added my Colab notebook\"  # Commit the changes\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nJpqW5yA87cy","executionInfo":{"status":"ok","timestamp":1706695088847,"user_tz":-120,"elapsed":3477,"user":{"displayName":"Adi Pasat","userId":"01384556905674102264"}},"outputId":"a18bb9aa-ed7a-4eb9-d5e7-92d2e62e16b1"},"id":"nJpqW5yA87cy","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: 'repo # Navigate to the repo directory'\n","/content/drive/My Drive/Colab Notebooks\n","[master bdae24a] Added my Colab notebook\n"," 1 file changed, 1 insertion(+)\n"," create mode 100644 Scrape_v3.ipynb\n"]}]},{"cell_type":"code","source":["!git config --global credential.helper store\n","!echo \"https://pasatsanduadrian:${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}@github.com\" > ~/.git-credentials\n"],"metadata":{"id":"89tJWy-W9V1M","executionInfo":{"status":"ok","timestamp":1706695367675,"user_tz":-120,"elapsed":255,"user":{"displayName":"Adi Pasat","userId":"01384556905674102264"}}},"id":"89tJWy-W9V1M","execution_count":60,"outputs":[]},{"cell_type":"code","source":["!rm -f ~/.git-credential-cache\n"],"metadata":{"id":"xmUqy4Sp-4x3","executionInfo":{"status":"ok","timestamp":1706695595651,"user_tz":-120,"elapsed":9,"user":{"displayName":"Adi Pasat","userId":"01384556905674102264"}}},"id":"xmUqy4Sp-4x3","execution_count":4,"outputs":[]},{"cell_type":"code","source":["!git config --global credential.helper cache\n","!git config --global credential.helper 'cache --timeout=3600'\n"],"metadata":{"id":"6z4HIgnu-e3v","executionInfo":{"status":"ok","timestamp":1706695618342,"user_tz":-120,"elapsed":440,"user":{"displayName":"Adi Pasat","userId":"01384556905674102264"}}},"id":"6z4HIgnu-e3v","execution_count":7,"outputs":[]},{"cell_type":"code","source":["!rm -rf '/content/drive/My Drive/Colab Notebooks/repo/'\n"],"metadata":{"id":"slRiaCM__EMO","executionInfo":{"status":"ok","timestamp":1706695717297,"user_tz":-120,"elapsed":278,"user":{"displayName":"Adi Pasat","userId":"01384556905674102264"}}},"id":"slRiaCM__EMO","execution_count":12,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/pasatsanduadrian/repo.git\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0wDzqqK-_i6e","executionInfo":{"status":"ok","timestamp":1706695743995,"user_tz":-120,"elapsed":262,"user":{"displayName":"Adi Pasat","userId":"01384556905674102264"}},"outputId":"0b1d024b-0f76-4d56-9d54-31f7c922c05f"},"id":"0wDzqqK-_i6e","execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'repo' already exists and is not an empty directory.\n"]}]},{"cell_type":"code","source":["%cd '/content/drive/My Drive/Colab Notebooks/'\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nKq01fWJ_hjP","executionInfo":{"status":"ok","timestamp":1706695786447,"user_tz":-120,"elapsed":312,"user":{"displayName":"Adi Pasat","userId":"01384556905674102264"}},"outputId":"74600ebc-baaf-4cab-d710-246aae5fe7a1"},"id":"nKq01fWJ_hjP","execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Colab Notebooks\n"]}]},{"cell_type":"code","source":["cd repo  # Navigate to the 'repo' directory\n","!git add Scrape_v3.ipynb  # Add the file you want to commit\n","!git commit -m \"Added my Colab notebook\"  # Commit the changes\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"Wnw_U66d_wkx","executionInfo":{"status":"error","timestamp":1706695818191,"user_tz":-120,"elapsed":266,"user":{"displayName":"Adi Pasat","userId":"01384556905674102264"}},"outputId":"1c29002f-2960-470b-ca43-d4fd96ed60f0"},"id":"Wnw_U66d_wkx","execution_count":18,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-18-e3b518d896ce>, line 1)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-e3b518d896ce>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    cd repo  # Navigate to the 'repo' directory\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}